---
title: ChatSeekrFlow
---

> [Seekr](https://www.seekr.com/)는 구조화되고 설명 가능하며 투명한 AI 상호 작용을 위한 AI 기반 솔루션을 제공합니다.

이 가이드는 Seekr [채팅 모델](/oss/langchain/models) 시작을 위한 간단한 개요를 제공합니다. 모든 `ChatSeekrFlow` 기능 및 구성에 대한 자세한 문서는 [API 레퍼런스](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.seekrflow.ChatSeekrFlow.html)를 참조하세요.

## 개요

`ChatSeekrFlow` 클래스는 SeekrFlow에 호스팅된 채팅 모델 엔드포인트를 래핑하여 LangChain 애플리케이션과의 원활한 통합을 가능하게 합니다.

### 통합 세부 정보

| Class | Package | Local | Serializable | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: |
| [ChatSeekrFlow](https://python.langchain.com/api_reference/community/chat_models/langchain_community.chat_models.seekrflow.ChatSeekrFlow.html) | [seekrai](https://python.langchain.com/docs/integrations/providers/seekr/) | ❌ | beta | ![PyPI - Downloads](https://img.shields.io/pypi/dm/seekrai?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/seekrai?style=flat-square&label=%20) |

### 모델 기능

| [도구 호출](/oss/langchain/tools/) | [구조화된 출력](/oss/langchain/structured-output) | JSON 모드 | [이미지 입력](/oss/langchain/messages#multimodal) | 오디오 입력 | 비디오 입력 | [토큰 수준 스트리밍](/oss/langchain/streaming/) | 네이티브 비동기 | [토큰 사용량](/oss/langchain/models#token-usage) | [로그 확률](/oss/langchain/models#log-probabilities) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ |

### 지원되는 메서드

`ChatSeekrFlow`는 **비동기 API를 제외한** `ChatModel`의 모든 메서드를 지원합니다.

### 엔드포인트 요구 사항

`ChatSeekrFlow`가 래핑하는 서빙 엔드포인트는 OpenAI 호환 채팅 입/출력 형식을 **반드시** 가져야 합니다. 다음에 사용할 수 있습니다:

1. **미세 조정된 Seekr 모델**
2. **사용자 정의 SeekrFlow 모델**
3. **Seekr의 검색 시스템을 사용하는 RAG 지원 모델**

비동기 사용에 대해서는 `AsyncChatSeekrFlow`를 참조하세요 (곧 출시 예정).

# LangChain에서 ChatSeekrFlow 시작하기

이 노트북은 LangChain에서 SeekrFlow를 채팅 모델로 사용하는 방법을 다룹니다.

## 설정

필요한 종속성이 설치되어 있는지 확인하세요:

```bash
pip install seekrai langchain langchain-community
```

요청을 인증하려면 Seekr의 API 키도 있어야 합니다.

```python
# Standard library
import getpass
import os

# Third-party
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage
from langchain_core.runnables import RunnableSequence

# OSS SeekrFlow integration
from langchain_seekrflow import ChatSeekrFlow
from seekrai import SeekrFlow
```

## API 키 설정

요청을 인증하려면 API 키를 환경 변수로 설정해야 합니다.

아래 셀을 실행하세요.

또는 쿼리를 실행하기 전에 수동으로 할당하세요:

```python
SEEKR_API_KEY = "your-api-key-here"
```

```python
os.environ["SEEKR_API_KEY"] = getpass.getpass("Enter your Seekr API key:")
```

## 인스턴스화

```python
os.environ["SEEKR_API_KEY"]
seekr_client = SeekrFlow(api_key=SEEKR_API_KEY)

llm = ChatSeekrFlow(
    client=seekr_client, model_name="meta-llama/Meta-Llama-3-8B-Instruct"
)
```

## 호출

```python
response = llm.invoke([HumanMessage(content="Hello, Seekr!")])
print(response.content)
```

```output
Hello there! I'm Seekr, nice to meet you! What brings you here today? Do you have a question, or are you looking for some help with something? I'm all ears (or rather, all text)!
```

## 체이닝

```python
prompt = ChatPromptTemplate.from_template("Translate to French: {text}")

chain: RunnableSequence = prompt | llm
result = chain.invoke({"text": "Good morning"})
print(result)
```

```output
content='The translation of "Good morning" in French is:\n\n"Bonne journée"' additional_kwargs={} response_metadata={}
```

```python
def test_stream():
    """Test synchronous invocation in streaming mode."""
    print("\n🔹 Testing Sync `stream()` (Streaming)...")

    for chunk in llm.stream([HumanMessage(content="Write me a haiku.")]):
        print(chunk.content, end="", flush=True)


# ✅ Ensure streaming is enabled
llm = ChatSeekrFlow(
    client=seekr_client,
    model_name="meta-llama/Meta-Llama-3-8B-Instruct",
    streaming=True,  # ✅ Enable streaming
)

# ✅ Run sync streaming test
test_stream()
```

```output
🔹 Testing Sync `stream()` (Streaming)...
Here is a haiku:

Golden sunset fades
Ripples on the quiet lake
Peaceful evening sky
```

## 오류 처리 및 디버깅

```python
# Define a minimal mock SeekrFlow client
class MockSeekrClient:
    """Mock SeekrFlow API client that mimics the real API structure."""

    class MockChat:
        """Mock Chat object with a completions method."""

        class MockCompletions:
            """Mock Completions object with a create method."""

            def create(self, *args, **kwargs):
                return {
                    "choices": [{"message": {"content": "Mock response"}}]
                }  # Mimic API response

        completions = MockCompletions()

    chat = MockChat()


def test_initialization_errors():
    """Test that invalid ChatSeekrFlow initializations raise expected errors."""

    test_cases = [
        {
            "name": "Missing Client",
            "args": {"client": None, "model_name": "seekrflow-model"},
            "expected_error": "SeekrFlow client cannot be None.",
        },
        {
            "name": "Missing Model Name",
            "args": {"client": MockSeekrClient(), "model_name": ""},
            "expected_error": "A valid model name must be provided.",
        },
    ]

    for test in test_cases:
        try:
            print(f"Running test: {test['name']}")
            faulty_llm = ChatSeekrFlow(**test["args"])

            # If no error is raised, fail the test
            print(f"❌ Test '{test['name']}' failed: No error was raised!")
        except Exception as e:
            error_msg = str(e)
            assert test["expected_error"] in error_msg, f"Unexpected error: {error_msg}"
            print(f"✅ Expected Error: {error_msg}")


# Run test
test_initialization_errors()
```

```output
Running test: Missing Client
✅ Expected Error: SeekrFlow client cannot be None.
Running test: Missing Model Name
✅ Expected Error: A valid model name must be provided.
```

## API 레퍼런스

- `ChatSeekrFlow` 클래스: [`langchain_seekrflow.ChatSeekrFlow`](https://github.com/benfaircloth/langchain-seekrflow/blob/main/langchain_seekrflow/seekrflow.py)
- PyPI 패키지: [`langchain-seekrflow`](https://pypi.org/project/langchain-seekrflow/)
