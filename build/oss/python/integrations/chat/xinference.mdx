---
title: ChatXinference
---

[Xinference](https://github.com/xorbitsai/inference)는 노트북에서도 LLM, 음성 인식 모델, 멀티모달 모델을 서빙할 수 있도록 설계된 강력하고 다재다능한 라이브러리입니다. chatglm, baichuan, whisper, vicuna, orca 등 GGML과 호환되는 다양한 모델을 지원합니다.

## 개요

### 통합 세부 정보

| Class | Package | Local | Serializable | [JS support] | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| ChatXinference| langchain-xinference | ✅ | ❌ | ✅ | ✅ | ✅ |

### 모델 기능

| [Tool calling](/oss/python/langchain/tools/) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming/) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :---: |:----------------------------------------------------:| :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ |                          ✅                           | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ |

## 설정

PyPI를 통해 `Xinference`를 설치하세요:

```python
%pip install -qU  "xinference[all]"
```

### Xinference를 로컬 또는 분산 클러스터에 배포

로컬 배포의 경우 `xinference`를 실행하세요.

클러스터에 Xinference를 배포하려면 먼저 `xinference-supervisor`를 사용하여 Xinference 슈퍼바이저를 시작하세요. -p 옵션으로 포트를 지정하고 -H 옵션으로 호스트를 지정할 수도 있습니다. 기본 포트는 8080이고 기본 호스트는 0.0.0.0입니다.

그런 다음 실행하려는 각 서버에서 `xinference-worker`를 사용하여 Xinference 워커를 시작하세요.

자세한 내용은 [Xinference](https://github.com/xorbitsai/inference)의 README 파일을 참조하세요.

### 래퍼

LangChain과 함께 Xinference를 사용하려면 먼저 모델을 실행해야 합니다. 명령줄 인터페이스(CLI)를 사용하여 다음과 같이 할 수 있습니다:

```python
%xinference launch -n vicuna-v1.3 -f ggmlv3 -q q4_0
```

```output
Model uid: 7167b2b0-2a04-11ee-83f0-d29396a3f064
```

사용할 모델 UID가 반환됩니다. 이제 LangChain과 함께 Xinference를 사용할 수 있습니다:

## 설치

LangChain Xinference 통합은 `langchain-xinference` 패키지에 있습니다:

```python
%pip install -qU langchain-xinference
```

구조화된 출력을 위해 최신 Xinference 버전을 사용하고 있는지 확인하세요.

## 인스턴스화

이제 모델 객체를 인스턴스화하고 채팅 완성을 생성할 수 있습니다:

```python
from langchain_xinference.chat_models import ChatXinference

llm = ChatXinference(
    server_url="your_server_url", model_uid="7167b2b0-2a04-11ee-83f0-d29396a3f064"
)

llm.invoke(
    "Q: where can we visit in the capital of France?",
    config={"max_tokens": 1024},
)
```

## 호출

```python
from langchain.messages import HumanMessage, SystemMessage
from langchain_xinference.chat_models import ChatXinference

llm = ChatXinference(
    server_url="your_server_url", model_uid="7167b2b0-2a04-11ee-83f0-d29396a3f064"
)

system_message = "You are a helpful assistant that translates English to French. Translate the user sentence."
human_message = "I love programming."

llm.invoke([HumanMessage(content=human_message), SystemMessage(content=system_message)])
```

## API 레퍼런스

모든 ChatXinference 기능과 구성에 대한 자세한 문서는 API 레퍼런스를 참조하세요: [github.com/TheSongg/langchain-xinference](https://github.com/TheSongg/langchain-xinference)

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/xinference.mdx)
</Callout>
