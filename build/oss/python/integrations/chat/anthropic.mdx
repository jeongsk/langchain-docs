---
title: ChatAnthropic
---

이 가이드는 Anthropic [채팅 모델](/oss/python/langchain/models) 시작하기에 대한 간략한 개요를 제공합니다. ChatAnthropic의 모든 기능과 설정에 대한 자세한 문서는 [API 레퍼런스](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)를 참조하세요.

Anthropic은 여러 채팅 모델을 제공합니다. 최신 모델과 비용, 컨텍스트 윈도우, 지원되는 입력 타입에 대한 정보는 [Anthropic 문서](https://docs.claude.com/en/docs/about-claude/models/overview)에서 확인할 수 있습니다.

<Info>
**AWS Bedrock 및 Google VertexAI**

특정 Anthropic 모델은 AWS Bedrock 및 Google VertexAI를 통해서도 액세스할 수 있습니다. 이러한 서비스를 통해 Anthropic 모델을 사용하려면 [ChatBedrock](/oss/python/integrations/chat/bedrock/) 및 [ChatVertexAI](/oss/python/integrations/chat/google_vertex_ai_palm/) 통합을 참조하세요.

</Info>

## 개요

### 통합 세부 정보

| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/anthropic) | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatAnthropic](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html) | [langchain-anthropic](https://python.langchain.com/api_reference/anthropic/index.html) | ❌ | beta | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-anthropic?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-anthropic?style=flat-square&label=%20) |

### 모델 기능

| [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming/) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |

## 설정

Anthropic 모델에 액세스하려면 Anthropic 계정을 생성하고, API 키를 발급받고, `langchain-anthropic` 통합 패키지를 설치해야 합니다.

### 자격 증명

[console.anthropic.com/](https://console.anthropic.com/)로 이동하여 Anthropic에 가입하고 API 키를 생성하세요. 완료한 후 ANTHROPIC_API_KEY 환경 변수를 설정하세요:

```python
import getpass
import os

if "ANTHROPIC_API_KEY" not in os.environ:
    os.environ["ANTHROPIC_API_KEY"] = getpass.getpass("Enter your Anthropic API key: ")
```

모델 호출의 자동 추적을 활성화하려면 [LangSmith](https://docs.smith.langchain.com/) API 키를 설정하세요:

```python
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
# os.environ["LANGSMITH_TRACING"] = "true"
```

### 설치

LangChain Anthropic 통합은 `langchain-anthropic` 패키지에 있습니다:

```python
%pip install -qU langchain-anthropic
```

<Info>
    **이 가이드에는 `langchain-anthropic>=0.3.13`이 필요합니다**
</Info>

## 인스턴스화

이제 모델 객체를 인스턴스화하고 채팅 완성을 생성할 수 있습니다:

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-3-5-haiku-latest",
    temperature=0,
    max_tokens=1024,
    timeout=None,
    max_retries=2,
    # other params...
)
```

## 호출

```python
messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
```

```output
AIMessage(content="J'adore la programmation.", response_metadata={'id': 'msg_018Nnu76krRPq8HvgKLW4F8T', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 29, 'output_tokens': 11}}, id='run-57e9295f-db8a-48dc-9619-babd2bedd891-0', usage_metadata={'input_tokens': 29, 'output_tokens': 11, 'total_tokens': 40})
```

```python
print(ai_msg.text)
```

```output
J'adore la programmation.
```

## 콘텐츠 블록

도구, [확장된 사고](#extended-thinking) 및 기타 기능을 사용할 때, 단일 Anthropic [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage)의 콘텐츠는 단일 문자열이거나 콘텐츠 블록 목록일 수 있습니다. 예를 들어, Anthropic 모델이 도구를 호출할 때, 도구 호출은 메시지 콘텐츠의 일부이며 (표준화된 `AIMessage.tool_calls`에도 노출됩니다):

```python
from langchain_anthropic import ChatAnthropic
from typing_extensions import Annotated

llm = ChatAnthropic(model="claude-3-5-haiku-latest")


def get_weather(
    location: Annotated[str, ..., "Location as city and state."]
) -> str:
    """Get the weather at a location."""
    return "It's sunny."


llm_with_tools = llm.bind_tools([get_weather])
response = llm_with_tools.invoke("Which city is hotter today: LA or NY?")
response.content
```

```output
[{'text': "I'll help you compare the temperatures of Los Angeles and New York by checking their current weather. I'll retrieve the weather for both cities.",
  'type': 'text'},
 {'id': 'toolu_01CkMaXrgmsNjTso7so94RJq',
  'input': {'location': 'Los Angeles, CA'},
  'name': 'get_weather',
  'type': 'tool_use'},
 {'id': 'toolu_01SKaTBk9wHjsBTw5mrPVSQf',
  'input': {'location': 'New York, NY'},
  'name': 'get_weather',
  'type': 'tool_use'}]
```

`content_blocks`를 사용하면 콘텐츠가 제공자 간에 일관된 표준 형식으로 렌더링됩니다:

```python
response.content_blocks
```

```output
[{'type': 'text',
  'text': "I'll help you compare the temperatures of Los Angeles and New York by checking their current weather. I'll retrieve the weather for both cities."},
 {'type': 'tool_call',
  'name': 'get_weather',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'toolu_01CkMaXrgmsNjTso7so94RJq'},
 {'type': 'tool_call',
  'name': 'get_weather',
  'args': {'location': 'New York, NY'},
  'id': 'toolu_01SKaTBk9wHjsBTw5mrPVSQf'}]
```

`.tool_calls` 속성을 사용하여 표준 형식으로 도구 호출에 특정적으로 액세스할 수도 있습니다:

```python
ai_msg.tool_calls
```

```output
[{'name': 'GetWeather',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'toolu_01Ddzj5PkuZkrjF4tafzu54A'},
 {'name': 'GetWeather',
  'args': {'location': 'New York, NY'},
  'id': 'toolu_012kz4qHZQqD4qg8sFPeKqpP'}]
```

## 멀티모달

Claude는 이미지 및 PDF 입력을 콘텐츠 블록으로 지원하며, Anthropic의 네이티브 형식([비전](https://docs.claude.com/en/docs/build-with-claude/vision#base64-encoded-image-example) 및 [PDF 지원](https://docs.claude.com/en/docs/build-with-claude/pdf-support) 문서 참조) 뿐만 아니라 LangChain의 [표준 형식](/oss/python/langchain/messages#multimodal)을 모두 지원합니다.

### Files API

Claude는 관리형 [Files API](https://docs.claude.com/en/docs/build-with-claude/files)를 통한 파일 상호작용도 지원합니다. 아래 예제를 참조하세요.

Files API는 Claude의 내장 코드 실행 도구와 함께 사용하기 위해 컨테이너에 파일을 업로드하는 데에도 사용할 수 있습니다. 자세한 내용은 아래 [코드 실행](#code-execution) 섹션을 참조하세요.

<Accordion title="Images">

```python
# Upload image

import anthropic

client = anthropic.Anthropic()
file = client.beta.files.upload(
    # Supports image/jpeg, image/png, image/gif, image/webp
    file=("image.png", open("/path/to/image.png", "rb"), "image/png"),
)
image_file_id = file.id


# Run inference
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    betas=["files-api-2025-04-14"],
)

input_message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Describe this image.",
        },
        {
            "type": "image",
            "file_id": image_file_id,
        },
    ],
}
llm.invoke([input_message])
```

</Accordion>

<Accordion title="PDFs">

```python
# Upload document

import anthropic

client = anthropic.Anthropic()
file = client.beta.files.upload(
    file=("document.pdf", open("/path/to/document.pdf", "rb"), "application/pdf"),
)
pdf_file_id = file.id


# Run inference
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    betas=["files-api-2025-04-14"],
)

input_message = {
    "role": "user",
    "content": [
        {"type": "text", "text": "Describe this document."},
        {"type": "file", "file_id": pdf_file_id}
    ],
}
llm.invoke([input_message])
```

</Accordion>

## 확장된 사고

일부 Claude 모델은 [확장된 사고](https://docs.claude.com/en/docs/build-with-claude/extended-thinking) 기능을 지원하며, 이는 최종 답변에 도달하기까지의 단계별 추론 과정을 출력합니다.

해당하는 모델은 Anthropic 가이드 [여기](https://docs.claude.com/en/docs/build-with-claude/extended-thinking)를 참조하세요.

확장된 사고를 사용하려면 `ChatAnthropic` 초기화 시 `thinking` 매개변수를 지정하세요. 호출 중에 `kwarg`로 전달할 수도 있습니다.

이 기능을 사용하려면 토큰 예산을 지정해야 합니다. 아래 사용 예제를 참조하세요:

```python
import json

from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",
    max_tokens=5000,
    thinking={"type": "enabled", "budget_tokens": 2000},
)

response = llm.invoke("What is the cube root of 50.653?")
print(json.dumps(response.content_blocks, indent=2))
```

```output
[
  {
    "type": "reasoning",
    "reasoning": "To find the cube root of 50.653, I need to find the value of $x$ such that $x^3 = 50.653$.\n\nI can try to estimate this first. \n$3^3 = 27$\n$4^3 = 64$\n\nSo the cube root of 50.653 will be somewhere between 3 and 4, but closer to 4.\n\nLet me try to compute this more precisely. I can use the cube root function:\n\ncube root of 50.653 = 50.653^(1/3)\n\nLet me calculate this:\n50.653^(1/3) ≈ 3.6998\n\nLet me verify:\n3.6998^3 ≈ 50.6533\n\nThat's very close to 50.653, so I'm confident that the cube root of 50.653 is approximately 3.6998.\n\nActually, let me compute this more precisely:\n50.653^(1/3) ≈ 3.69981\n\nLet me verify once more:\n3.69981^3 ≈ 50.652998\n\nThat's extremely close to 50.653, so I'll say that the cube root of 50.653 is approximately 3.69981.",
    "extras": {"signature": "ErUBCkYIBxgCIkB0UjV..."}
  },
  {
    "text": "The cube root of 50.653 is approximately 3.6998.\n\nTo verify: 3.6998³ = 50.6530, which is very close to our original number.",
    "type": "text"
  }
]
```

## 프롬프트 캐싱

Anthropic은 메시지, 도구 정의, 도구 결과, 이미지 및 문서를 포함한 [프롬프트 요소의 캐싱](https://docs.claude.com/en/docs/build-with-claude/prompt-caching)을 지원합니다. 이를 통해 대용량 문서, 지침, [퓨샷 문서](/langsmith/create-few-shot-evaluators) 및 기타 데이터를 재사용하여 지연 시간과 비용을 줄일 수 있습니다.

프롬프트 요소에 캐싱을 활성화하려면 `cache_control` 키를 사용하여 관련 콘텐츠 블록을 표시하세요. 아래 예제를 참조하세요:

### 메시지

```python
import requests
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-7-sonnet-20250219")

# Pull LangChain readme
get_response = requests.get(
    "https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
)
readme = get_response.text

messages = [
    {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": "You are a technology expert.",
            },
            {
                "type": "text",
                "text": f"{readme}",
                "cache_control": {"type": "ephemeral"},  # [!code highlight]
            },
        ],
    },
    {
        "role": "user",
        "content": "What's LangChain, according to its README?",
    },
]

response_1 = llm.invoke(messages)
response_2 = llm.invoke(messages)

usage_1 = response_1.usage_metadata["input_token_details"]
usage_2 = response_2.usage_metadata["input_token_details"]

print(f"First invocation:\n{usage_1}")
print(f"\nSecond:\n{usage_2}")
```

```output
First invocation:
{'cache_read': 0, 'cache_creation': 1458}

Second:
{'cache_read': 1458, 'cache_creation': 0}
```

<Tip>
**확장된 캐싱**

    캐시 수명은 기본적으로 5분입니다. 이것이 너무 짧으면 `"extended-cache-ttl-2025-04-11"` 베타 헤더를 활성화하여 1시간 캐싱을 적용할 수 있습니다:

    ```python
    llm = ChatAnthropic(
        model="claude-3-7-sonnet-20250219",
        betas=["extended-cache-ttl-2025-04-11"],  # [!code highlight]
    )
    ```
    그리고 `"cache_control": {"type": "ephemeral", "ttl": "1h"}`를 지정합니다.

    캐시된 토큰 수의 세부 정보는 응답의 `usage_metadata`의 `InputTokenDetails`에 포함됩니다:

    ```python
    response = llm.invoke(messages)
    response.usage_metadata
    ```
    ```
    {
        "input_tokens": 1500,
        "output_tokens": 200,
        "total_tokens": 1700,
        "input_token_details": {
            "cache_read": 0,
            "cache_creation": 1000,
            "ephemeral_1h_input_tokens": 750,
            "ephemeral_5m_input_tokens": 250,
        }
    }
    ```

</Tip>

### 도구

```python
from langchain_anthropic import convert_to_anthropic_tool
from langchain.tools import tool

# For demonstration purposes, we artificially expand the
# tool description.
description = (
    f"Get the weather at a location. By the way, check out this readme: {readme}"
)


@tool(description=description)
def get_weather(location: str) -> str:
    return "It's sunny."


# Enable caching on the tool
weather_tool = convert_to_anthropic_tool(get_weather)  # [!code highlight]
weather_tool["cache_control"] = {"type": "ephemeral"}  # [!code highlight]

llm = ChatAnthropic(model="claude-3-7-sonnet-20250219")
llm_with_tools = llm.bind_tools([weather_tool])
query = "What's the weather in San Francisco?"

response_1 = llm_with_tools.invoke(query)
response_2 = llm_with_tools.invoke(query)

usage_1 = response_1.usage_metadata["input_token_details"]
usage_2 = response_2.usage_metadata["input_token_details"]

print(f"First invocation:\n{usage_1}")
print(f"\nSecond:\n{usage_2}")
```

```output
First invocation:
{'cache_read': 0, 'cache_creation': 1809}

Second:
{'cache_read': 1809, 'cache_creation': 0}
```

### 대화형 애플리케이션의 증분 캐싱

프롬프트 캐싱은 [다중 턴 대화](https://docs.claude.com/en/docs/build-with-claude/prompt-caching#continuing-a-multi-turn-conversation)에서 중복 처리 없이 이전 메시지의 컨텍스트를 유지하는 데 사용할 수 있습니다.

마지막 메시지를 `cache_control`로 표시하여 증분 캐싱을 활성화할 수 있습니다. Claude는 후속 메시지에 대해 이전에 캐시된 가장 긴 접두사를 자동으로 사용합니다.

아래에서 이 기능을 통합하는 간단한 챗봇을 구현합니다. LangChain [챗봇 튜토리얼](/oss/python/langchain/quickstart)을 따르지만, 각 사용자 메시지의 마지막 콘텐츠 블록을 `cache_control`로 자동으로 표시하는 사용자 정의 [리듀서](/oss/python/langgraph/graph-api#reducers)를 추가합니다. 아래를 참조하세요:

```python
import requests
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph, add_messages
from typing_extensions import Annotated, TypedDict

llm = ChatAnthropic(model="claude-3-7-sonnet-20250219")

# Pull LangChain readme
get_response = requests.get(
    "https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
)
readme = get_response.text


def messages_reducer(left: list, right: list) -> list:
    # Update last user message
    for i in range(len(right) - 1, -1, -1):
        if right[i].type == "human":
            right[i].content[-1]["cache_control"] = {"type": "ephemeral"}
            break

    return add_messages(left, right)


class State(TypedDict):
    messages: Annotated[list, messages_reducer]


workflow = StateGraph(state_schema=State)


# Define the function that calls the model
def call_model(state: State):
    response = llm.invoke(state["messages"])
    return {"messages": [response]}


# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

```python
from langchain.messages import HumanMessage

config = {"configurable": {"thread_id": "abc123"}}

query = "Hi! I'm Bob."

input_message = HumanMessage([{"type": "text", "text": query}])
output = app.invoke({"messages": [input_message]}, config)
output["messages"][-1].pretty_print()
print(f"\n{output['messages'][-1].usage_metadata['input_token_details']}")
```

```output
================================== Ai Message ==================================

Hello, Bob! It's nice to meet you. How are you doing today? Is there something I can help you with?

{'cache_read': 0, 'cache_creation': 0}
```

```python
query = f"Check out this readme: {readme}"

input_message = HumanMessage([{"type": "text", "text": query}])
output = app.invoke({"messages": [input_message]}, config)
output["messages"][-1].pretty_print()
print(f"\n{output['messages'][-1].usage_metadata['input_token_details']}")
```

```output
================================== Ai Message ==================================

I can see you've shared the README from the LangChain GitHub repository. This is the documentation for LangChain, which is a popular framework for building applications powered by Large Language Models (LLMs). Here's a summary of what the README contains:

LangChain is:
- A framework for developing LLM-powered applications
- Helps chain together components and integrations to simplify AI application development
- Provides a standard interface for models, embeddings, vector stores, etc.

Key features/benefits:
- Real-time data augmentation (connect LLMs to diverse data sources)
- Model interoperability (swap models easily as needed)
- Large ecosystem of integrations

The LangChain ecosystem includes:
- LangSmith - For evaluations and observability
- LangGraph - For building complex agents with customizable architecture
- LangSmith - For deployment and scaling of agents

The README also mentions installation instructions (`pip install -U langchain`) and links to various resources including tutorials, how-to guides, conceptual guides, and API references.

Is there anything specific about LangChain you'd like to know more about, Bob?

{'cache_read': 0, 'cache_creation': 1498}
```

```python
query = "What was my name again?"

input_message = HumanMessage([{"type": "text", "text": query}])
output = app.invoke({"messages": [input_message]}, config)
output["messages"][-1].pretty_print()
print(f"\n{output['messages'][-1].usage_metadata['input_token_details']}")
```

```output
================================== Ai Message ==================================

Your name is Bob. You introduced yourself at the beginning of our conversation.

{'cache_read': 1498, 'cache_creation': 269}
```

[LangSmith 트레이스](https://smith.langchain.com/public/4d0584d8-5f9e-4b91-8704-93ba2ccf416a/r)에서 "raw output"을 토글하면 `cache_control` 키를 포함하여 채팅 모델로 전송되는 정확한 메시지를 볼 수 있습니다.

## 토큰 효율적 도구 사용

Anthropic은 (베타) [토큰 효율적 도구 사용](https://docs.claude.com/en/docs/agents-and-tools/tool-use/token-efficient-tool-use) 기능을 지원합니다. 이를 사용하려면 모델 인스턴스화 시 관련 베타 헤더를 지정하세요.

```python
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

llm = ChatAnthropic(
    model="claude-3-7-sonnet-20250219",
    betas=["token-efficient-tools-2025-02-19"],  # [!code highlight]
)


@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return "It's sunny."


llm_with_tools = llm.bind_tools([get_weather])
response = llm_with_tools.invoke("What's the weather in San Francisco?")
print(response.tool_calls)
print(f"\nTotal tokens: {response.usage_metadata['total_tokens']}")
```

```output
[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01EoeE1qYaePcmNbUvMsWtmA', 'type': 'tool_call'}]

Total tokens: 408
```

## 인용

Anthropic은 사용자가 제공한 소스 문서를 기반으로 Claude가 답변에 컨텍스트를 첨부할 수 있게 하는 [인용](https://docs.claude.com/en/docs/build-with-claude/citations) 기능을 지원합니다. `"citations": {"enabled": True}`가 설정된 [document](https://docs.claude.com/en/docs/build-with-claude/citations#document-types) 또는 `search result` 콘텐츠 블록이 쿼리에 포함되면 Claude는 응답에서 인용을 생성할 수 있습니다.

### 간단한 예제

이 예제에서는 [일반 텍스트 문서](https://docs.claude.com/en/docs/build-with-claude/citations#plain-text-documents)를 전달합니다. 백그라운드에서 Claude는 입력 텍스트를 문장으로 [자동으로 청크](https://docs.claude.com/en/docs/build-with-claude/citations#plain-text-documents)하며, 이는 인용을 생성할 때 사용됩니다.

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-5-haiku-latest")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "document",
                "source": {
                    "type": "text",
                    "media_type": "text/plain",
                    "data": "The grass is green. The sky is blue.",
                },
                "title": "My Document",
                "context": "This is a trustworthy document.",
                "citations": {"enabled": True},
            },
            {"type": "text", "text": "What color is the grass and sky?"},
        ],
    }
]
response = llm.invoke(messages)
response.content
```

```output
[{'text': 'Based on the document, ', 'type': 'text'},
 {'text': 'the grass is green',
  'type': 'text',
  'citations': [{'type': 'char_location',
    'cited_text': 'The grass is green. ',
    'document_index': 0,
    'document_title': 'My Document',
    'start_char_index': 0,
    'end_char_index': 20}]},
 {'text': ', and ', 'type': 'text'},
 {'text': 'the sky is blue',
  'type': 'text',
  'citations': [{'type': 'char_location',
    'cited_text': 'The sky is blue.',
    'document_index': 0,
    'document_title': 'My Document',
    'start_char_index': 20,
    'end_char_index': 36}]},
 {'text': '.', 'type': 'text'}]
```

### 도구 결과에서 (에이전틱 RAG)

<Info>
    **`langchain-anthropic>=0.3.17` 필요**
</Info>

Claude는 지식 베이스 또는 기타 사용자 정의 소스에 대한 쿼리의 인용 가능한 결과를 나타내는 [search_result](https://docs.claude.com/en/docs/build-with-claude/search-results) 콘텐츠 블록을 지원합니다. 이러한 콘텐츠 블록은 (위 예제와 같이) 최상위 수준과 도구 결과 내에서 claude에 전달할 수 있습니다. 이를 통해 Claude는 도구 호출 결과를 사용하여 응답의 요소를 인용할 수 있습니다.

도구 호출에 대한 응답으로 검색 결과를 전달하려면, Anthropic의 네이티브 형식으로 `search_result` 콘텐츠 블록 목록을 반환하는 도구를 정의하세요. 예를 들어:

```python
def retrieval_tool(query: str) -> list[dict]:
    """Access my knowledge base."""

    # Run a search (e.g., with a LangChain vector store)
    results = vector_store.similarity_search(query=query, k=2)

    # Package results into search_result blocks
    return [
        {
            "type": "search_result",
            # Customize fields as desired, using document metadata or otherwise
            "title": "My Document Title",
            "source": "Source description or provenance",
            "citations": {"enabled": True},
            "content": [{"type": "text", "text": doc.page_content}],
        }
        for doc in results
    ]
```

<Accordion title="LangGraph를 사용한 전체 예제">

여기서는 샘플 문서로 LangChain [벡터 스토어](/oss/python/integrations/vectorstores/)를 채우고 해당 문서를 쿼리하는 도구를 Claude에 제공하는 전체 예제를 보여줍니다.
여기에서 도구는 검색 쿼리와 `category` 문자열 리터럴을 가져오지만, 유효한 도구 서명을 사용할 수 있습니다.

```python
from typing import Literal

from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent


# Set up vector store
embeddings = init_embeddings("openai:text-embedding-3-small")
vector_store = InMemoryVectorStore(embeddings)

document_1 = Document(
    id="1",
    page_content=(
        "To request vacation days, submit a leave request form through the "
        "HR portal. Approval will be sent by email."
    ),
    metadata={
        "category": "HR Policy",
        "doc_title": "Leave Policy",
        "provenance": "Leave Policy - page 1",
    },
)
document_2 = Document(
    id="2",
    page_content="Managers will review vacation requests within 3 business days.",
    metadata={
        "category": "HR Policy",
        "doc_title": "Leave Policy",
        "provenance": "Leave Policy - page 2",
    },
)
document_3 = Document(
    id="3",
    page_content=(
        "Employees with over 6 months tenure are eligible for 20 paid vacation days "
        "per year."
    ),
    metadata={
        "category": "Benefits Policy",
        "doc_title": "Benefits Guide 2025",
        "provenance": "Benefits Policy - page 1",
    },
)

documents = [document_1, document_2, document_3]
vector_store.add_documents(documents=documents)


# Define tool
async def retrieval_tool(
    query: str, category: Literal["HR Policy", "Benefits Policy"]
) -> list[dict]:
    """Access my knowledge base."""

    def _filter_function(doc: Document) -> bool:
        return doc.metadata.get("category") == category

    results = vector_store.similarity_search(
        query=query, k=2, filter=_filter_function
    )

    return [
        {
            "type": "search_result",
            "title": doc.metadata["doc_title"],
            "source": doc.metadata["provenance"],
            "citations": {"enabled": True},
            "content": [{"type": "text", "text": doc.page_content}],
        }
        for doc in results
    ]



# Create agent
llm = init_chat_model("anthropic:claude-3-5-haiku-latest")

checkpointer = InMemorySaver()
agent = create_agent(llm, [retrieval_tool], checkpointer=checkpointer)


# Invoke on a query
config = {"configurable": {"thread_id": "session_1"}}

input_message = {
    "role": "user",
    "content": "How do I request vacation days?",
}
async for step in agent.astream(
    {"messages": [input_message]},
    config,
    stream_mode="values",
):
    step["messages"][-1].pretty_print()
```

</Accordion>

### 텍스트 분할기와 함께 사용

Anthropic은 [사용자 정의 문서](https://docs.claude.com/en/docs/build-with-claude/citations#custom-content-documents) 타입을 사용하여 자체 분할을 지정할 수도 있습니다. LangChain [텍스트 분할기](/oss/python/integrations/splitters/)를 사용하여 이 목적으로 의미 있는 분할을 생성할 수 있습니다. 아래 예제에서는 LangChain README(마크다운 문서)를 분할하고 컨텍스트로 Claude에 전달합니다:

```python
import requests
from langchain_anthropic import ChatAnthropic
from langchain_text_splitters import MarkdownTextSplitter


def format_to_anthropic_documents(documents: list[str]):
    return {
        "type": "document",
        "source": {
            "type": "content",
            "content": [{"type": "text", "text": document} for document in documents],
        },
        "citations": {"enabled": True},
    }


# Pull readme
get_response = requests.get(
    "https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
)
readme = get_response.text

# Split into chunks
splitter = MarkdownTextSplitter(
    chunk_overlap=0,
    chunk_size=50,
)
documents = splitter.split_text(readme)

# Construct message
message = {
    "role": "user",
    "content": [
        format_to_anthropic_documents(documents),
        {"type": "text", "text": "Give me a link to LangChain's tutorials."},
    ],
}

# Query LLM
llm = ChatAnthropic(model="claude-3-5-haiku-latest")
response = llm.invoke([message])

response.content
```

```output
[{'text': "You can find LangChain's tutorials at https://python.langchain.com/docs/tutorials/\n\nThe tutorials section is recommended for those looking to build something specific or who prefer a hands-on learning approach. It's considered the best place to get started with LangChain.",
  'type': 'text',
  'citations': [{'type': 'content_block_location',
    'cited_text': "[Tutorials](https://python.langchain.com/docs/tutorials/):If you're looking to build something specific orare more of a hands-on learner, check out ourtutorials. This is the best place to get started.",
    'document_index': 0,
    'document_title': None,
    'start_block_index': 243,
    'end_block_index': 248}]}]
```

## 컨텍스트 관리

Anthropic은 모델의 컨텍스트 윈도우를 자동으로 관리하는 (예: 도구 결과를 지워서) 컨텍스트 편집 기능을 지원합니다.

자세한 내용 및 구성 옵션은 [Anthropic 문서](https://docs.claude.com/en/docs/build-with-claude/context-editing)를 참조하세요.

<Info>
    **컨텍스트 관리는 `langchain-anthropic>=0.3.21`부터 지원됩니다**
</Info>

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",
    betas=["context-management-2025-06-27"],
    context_management={"edits": [{"type": "clear_tool_uses_20250919"}]},
)
llm_with_tools = llm.bind_tools([{"type": "web_search_20250305", "name": "web_search"}])
response = llm_with_tools.invoke("Search for recent developments in AI")
```

## 내장 도구

Anthropic은 다양한 [내장 도구](https://docs.claude.com/en/docs/agents-and-tools/tool-use/text-editor-tool)를 지원하며, [일반적인 방법](/oss/python/langchain/tools/)으로 모델에 바인딩할 수 있습니다. Claude는 도구에 대한 내부 스키마를 준수하는 도구 호출을 생성합니다:

### 웹 검색

Claude는 [웹 검색 도구](https://docs.claude.com/en/docs/agents-and-tools/tool-use/web-search-tool)를 사용하여 검색을 실행하고 인용으로 응답을 뒷받침할 수 있습니다.

<Info>
    **웹 검색 도구는 `langchain-anthropic>=0.3.13`부터 지원됩니다**
</Info>

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

tool = {"type": "web_search_20250305", "name": "web_search", "max_uses": 3}
llm_with_tools = llm.bind_tools([tool])

response = llm_with_tools.invoke("How do I update a web app to TypeScript 5.5?")
```

### 웹 페칭

Claude는 [웹 페칭 도구](https://docs.claude.com/en/docs/agents-and-tools/tool-use/web-fetch-tool)를 사용하여 검색을 실행하고 인용으로 응답을 뒷받침할 수 있습니다.

from langchain_anthropic import ChatAnthropic

```python
llm = ChatAnthropic(
    model="claude-3-5-haiku-latest",
    betas=["web-fetch-2025-09-10"],  # Enable web fetch beta
)

tool = {"type": "web_fetch_20250910", "name": "web_fetch", "max_uses": 3}
llm_with_tools = llm.bind_tools([tool])

response = llm_with_tools.invoke(
    "Please analyze the content at https://example.com/article"
)
```

<Warning>
    웹 페칭을 사용하려면 `'web-fetch-2025-09-10'` 베타 헤더를 추가해야 합니다.
</Warning>

### 코드 실행

Claude는 [코드 실행 도구](https://docs.claude.com/en/docs/agents-and-tools/tool-use/code-execution-tool)를 사용하여 샌드박스 환경에서 Python 코드를 실행할 수 있습니다.

<Info>
    **코드 실행은 `langchain-anthropic>=0.3.14`부터 지원됩니다**
</Info>

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    betas=["code-execution-2025-05-22"],
)

tool = {"type": "code_execution_20250522", "name": "code_execution"}
llm_with_tools = llm.bind_tools([tool])

response = llm_with_tools.invoke(
    "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
)
```

<Accordion title="Files API와 함께 사용">

Files API를 사용하면 Claude는 데이터 분석 및 기타 목적으로 파일에 액세스하는 코드를 작성할 수 있습니다. 아래 예제를 참조하세요:

```python
# Upload file

import anthropic

client = anthropic.Anthropic()
file = client.beta.files.upload(
    file=open("/path/to/sample_data.csv", "rb")
)
file_id = file.id


# Run inference
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    betas=["code-execution-2025-05-22"],
)

tool = {"type": "code_execution_20250522", "name": "code_execution"}
llm_with_tools = llm.bind_tools([tool])

input_message = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Please plot these data and tell me what you see.",
        },
        {
            "type": "container_upload",
            "file_id": file_id,
        },
    ]
}
llm_with_tools.invoke([input_message])
```

Claude는 코드 실행의 일부로 파일을 생성할 수 있습니다. Files API를 사용하여 이러한 파일에 액세스할 수 있습니다:

```python
# Take all file outputs for demonstration purposes
file_ids = []
for block in response.content:
    if block["type"] == "code_execution_tool_result":
        file_ids.extend(
            content["file_id"]
            for content in block.get("content", {}).get("content", [])
            if "file_id" in content
        )

for i, file_id in enumerate(file_ids):
    file_content = client.beta.files.download(file_id)
    file_content.write_to_file(f"/path/to/file_{i}.png")
```

</Accordion>

### 메모리 도구

Claude는 대화 스레드 간 컨텍스트의 클라이언트 측 저장 및 검색을 위한 메모리 도구를 지원합니다. 자세한 내용은 [여기](https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool) 문서를 참조하세요.

<Info>
    **Anthropic의 내장 메모리 도구는 `langchain-anthropic>=0.3.21`부터 지원됩니다**
</Info>

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-sonnet-4-5-20250929",
    betas=["context-management-2025-06-27"],
)
llm_with_tools = llm.bind_tools([{"type": "memory_20250818", "name": "memory"}])

response = llm_with_tools.invoke("What are my interests?")
```

### 원격 MCP

Claude는 원격 MCP 서버에 대한 모델 생성 호출을 위해 [MCP 커넥터 도구](https://docs.claude.com/en/docs/agents-and-tools/mcp-connector)를 사용할 수 있습니다.

<Info>
    **원격 MCP는 `langchain-anthropic>=0.3.14`부터 지원됩니다**
</Info>

```python
from langchain_anthropic import ChatAnthropic

mcp_servers = [
    {
        "type": "url",
        "url": "https://mcp.deepwiki.com/mcp",
        "name": "deepwiki",
        "tool_configuration": {  # optional configuration
            "enabled": True,
            "allowed_tools": ["ask_question"],
        },
        "authorization_token": "PLACEHOLDER",  # optional authorization
    }
]

llm = ChatAnthropic(
    model="claude-sonnet-4-20250514",
    betas=["mcp-client-2025-04-04"],
    mcp_servers=mcp_servers,
)

response = llm.invoke(
    "What transport protocols does the 2025-03-26 version of the MCP "
    "spec (modelcontextprotocol/modelcontextprotocol) support?"
)
```

### 텍스트 에디터

텍스트 에디터 도구는 텍스트 파일을 보고 수정하는 데 사용할 수 있습니다. 자세한 내용은 [여기](https://docs.claude.com/en/docs/agents-and-tools/tool-use/text-editor-tool) 문서를 참조하세요.

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-7-sonnet-20250219")

tool = {"type": "text_editor_20250124", "name": "str_replace_editor"}
llm_with_tools = llm.bind_tools([tool])

response = llm_with_tools.invoke(
    "There's a syntax error in my primes.py file. Can you help me fix it?"
)
print(response.text)
response.tool_calls
```

```output
I'd be happy to help you fix the syntax error in your primes.py file. First, let's look at the current content of the file to identify the error.
```

```output
[{'name': 'str_replace_editor',
  'args': {'command': 'view', 'path': '/repo/primes.py'},
  'id': 'toolu_01VdNgt1YV7kGfj9LFLm6HyQ',
  'type': 'tool_call'}]
```

## API 레퍼런스

ChatAnthropic의 모든 기능과 설정에 대한 자세한 문서는 API 레퍼런스를 참조하세요: [python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html](https://python.langchain.com/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html)

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/anthropic.mdx)
</Callout>
