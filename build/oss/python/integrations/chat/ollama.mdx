---
title: ChatOllama
---

[Ollama](https://ollama.com/)를 사용하면 `got-oss`와 같은 오픈 소스 대규모 언어 모델을 로컬로 실행할 수 있습니다.

`ollama`는 Modelfile로 정의된 단일 패키지에 모델 가중치, 구성 및 데이터를 번들로 제공합니다.

GPU 사용을 포함한 설정 및 구성 세부 정보를 최적화합니다.

지원되는 모델 및 모델 변형의 전체 목록은 [Ollama model library](https://github.com/jmorganca/ollama#model-library)를 참조하세요.

## 개요

### 통합 세부정보

| Class | Package | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/ollama) | Downloads | Version |
| :--- | :--- | :---: | :---: |  :---: | :---: | :---: |
| [ChatOllama](https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html#chatollama) | [langchain-ollama](https://python.langchain.com/api_reference/ollama/index.html) | ✅ | ❌ | ✅ | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-ollama?style=flat-square&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-ollama?style=flat-square&label=%20) |

### 모델 기능

| [Tool calling](/oss/python/langchain/tools/) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming/) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :---: |:----------------------------------------------------:| :---: | :---: |  :---: | :---: | :---: | :---: | :---: | :---: |
| ✅ |                          ✅                           | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ |

## 설정

먼저 [다음 지침](https://github.com/ollama/ollama?tab=readme-ov-file#ollama)에 따라 로컬 Ollama 인스턴스를 설정하고 실행하세요:

* WSL(Windows Subsystem for Linux), macOS 및 Linux를 포함하여 사용 가능한 지원 플랫폼에 Ollama를 [다운로드](https://ollama.ai/download)하고 설치하세요
  * macOS 사용자는 `brew install ollama`로 설치하고 `brew services start ollama`로 시작할 수 있습니다
* `ollama pull <name-of-model>`을 통해 사용 가능한 LLM 모델을 가져오세요
  * [모델 라이브러리](https://ollama.ai/library)를 통해 사용 가능한 모델 목록을 확인하세요
  * 예: `ollama pull gpt-oss:20b`
* 이렇게 하면 모델의 기본 태그 버전이 다운로드됩니다. 일반적으로 기본값은 최신의 가장 작은 크기 매개변수 모델을 가리킵니다.

> Mac에서 모델은 `~/.ollama/models`에 다운로드됩니다
>
> Linux(또는 WSL)에서 모델은 `/usr/share/ollama/.ollama/models`에 저장됩니다

* `ollama pull gpt-oss:20b`와 같이 관심 있는 모델의 정확한 버전을 지정하세요([`Vicuna`의 다양한 태그](https://ollama.ai/library/vicuna/tags)를 이 인스턴스에서 확인하세요)
* 모든 pull된 모델을 보려면 `ollama list`를 사용하세요
* 명령줄에서 직접 모델과 채팅하려면 `ollama run <name-of-model>`을 사용하세요
* 더 많은 명령은 [Ollama documentation](https://github.com/ollama/ollama/blob/main/docs/README.md)을 참조하세요. 터미널에서 `ollama help`를 실행하여 사용 가능한 명령을 볼 수 있습니다.

모델 호출의 자동 추적을 활성화하려면 [LangSmith](https://docs.smith.langchain.com/) API 키를 설정하세요:

```python
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
```

### 설치

LangChain Ollama 통합은 `langchain-ollama` 패키지에 있습니다:

```python
%pip install -qU langchain-ollama
```

<Warning>
최신 Ollama 버전을 사용하고 있는지 확인하세요!
</Warning>

다음을 실행하여 업데이트하세요:

```python
%pip install -U ollama
```

## 인스턴스화

이제 모델 객체를 인스턴스화하고 채팅 완성을 생성할 수 있습니다:

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1",
    temperature=0,
    # other params...
)
```

## 호출

```python
messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
```

```output
AIMessage(content='The translation of "I love programming" in French is:\n\n"J\'adore le programmation."', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-06-25T18:43:00.483666Z', 'done': True, 'done_reason': 'stop', 'total_duration': 619971208, 'load_duration': 27793125, 'prompt_eval_count': 35, 'prompt_eval_duration': 36354583, 'eval_count': 22, 'eval_duration': 555182667, 'model_name': 'llama3.1'}, id='run--348bb5ef-9dd9-4271-bc7e-a9ddb54c28c1-0', usage_metadata={'input_tokens': 35, 'output_tokens': 22, 'total_tokens': 57})
```

```python
print(ai_msg.content)
```

```output
The translation of "I love programming" in French is:

"J'adore le programmation."
```

## 도구 호출

[Ollama tool calling](https://ollama.com/blog/tool-support)은 OpenAI 호환 웹 서버 사양을 사용하며 [여기](/oss/python/langchain/tools/)에 설명된 대로 기본 `BaseChatModel.bind_tools()` 메서드와 함께 사용할 수 있습니다.

[tool calling](https://ollama.com/search?&c=tools)을 지원하는 ollama 모델을 선택해야 합니다.

`gpt-oss`와 같이 [도구 사용을 위해 미세 조정된 LLM](https://ollama.com/search?&c=tools)과 함께 [tool calling](/oss/python/langchain/tools/)을 사용할 수 있습니다:

```
ollama pull gpt-oss:20b
```

사용자 정의 도구 생성에 대한 세부 사항은 [이 가이드](/oss/python/langchain/tools#customize-tool-properties)에서 확인할 수 있습니다. 아래에서는 일반 python 함수에 [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) 데코레이터를 사용하여 도구를 생성하는 방법을 보여줍니다.

```python
from typing import List

from langchain.messages import AIMessage
from langchain.tools import tool
from langchain_ollama import ChatOllama


@tool
def validate_user(user_id: int, addresses: List[str]) -> bool:
    """Validate user using historical addresses.

    Args:
        user_id (int): the user ID.
        addresses (List[str]): Previous addresses as a list of strings.
    """
    return True


llm = ChatOllama(
    model="gpt-oss:20b",
    validate_model_on_init=True,
    temperature=0,
).bind_tools([validate_user])

result = llm.invoke(
    "Could you validate user 123? They previously lived at "
    "123 Fake St in Boston MA and 234 Pretend Boulevard in "
    "Houston TX."
)

if isinstance(result, AIMessage) and result.tool_calls:
    print(result.tool_calls)
```

```output
[{'name': 'validate_user', 'args': {'addresses': ['123 Fake St, Boston, MA', '234 Pretend Boulevard, Houston, TX'], 'user_id': '123'}, 'id': 'aef33a32-a34b-4b37-b054-e0d85584772f', 'type': 'tool_call'}]
```

## 멀티모달

Ollama는 [gemma3](https://ollama.com/library/gemma3)와 같은 멀티모달 LLM에 대한 제한적인 지원을 제공합니다.

멀티모달을 지원하려면 최신 버전이 있도록 Ollama를 업데이트해야 합니다.

```python
%pip install pillow
```

```python
import base64
from io import BytesIO

from IPython.display import HTML, display
from PIL import Image


def convert_to_base64(pil_image):
    """
    Convert PIL images to Base64 encoded strings

    :param pil_image: PIL image
    :return: Re-sized Base64 string
    """

    buffered = BytesIO()
    pil_image.save(buffered, format="JPEG")  # You can change the format if needed
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    return img_str


def plt_img_base64(img_base64):
    """
    Disply base64 encoded string as image

    :param img_base64:  Base64 string
    """
    # Create an HTML img tag with the base64 string as the source
    image_html = f'<img src="data:image/jpeg;base64,{img_base64}" />'
    # Display the image by rendering the HTML
    display(HTML(image_html))


file_path = "../../../static/img/ollama_example_img.jpg"
pil_image = Image.open(file_path)

image_b64 = convert_to_base64(pil_image)
plt_img_base64(image_b64)
```

```html
<img src="data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAIcA8ADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3eilpK0ICiiigAooooASilpKACiiigAooooAMUlLRTASilpKACkpaKAEooooEFJS0UDEopcUlMBKKWigBKKMUUCCiiigYUUUUAFFFFABRRRQAUlLRQAlFLikoAKKKKAEopaSmAUUUUAFFFFABRRRQMKKSlpAJRS0lAhKKWigYlFLRTASiiigAooooAKKKKACiiigBKKWkoAKKKKAEopaKACd6WiigYYpKWigBKKWkpgJRS0lABRRRQMSiloxTASiiigApKWigBKKWkoAKSlopgJRRRQMKSlooASiiigBMUUtFAxKQilxRTEJRS0lAwxSUtFACUUUUAFJS0UxiUYpaSgBKKWkoASjFLRTGNopaKAEopaSgAxSUtFMDZoooriMBKKWkpgFFFFABRRRQAUlLRQAlFLijFACUUUUAFFFFACUUtFMBKSlooEJRS4pKACiiigYlJmnUlMAoNFFACUtFGKAEooooAKKKKACiiigAooooASig0uKAEooooAKSlooASilpKYBRRRQAUUUUAFFFFABRiiigYlGKWjFACYpKWimAlFFFABRRRQAUd6KKACijvRQAlFLSUAFFFFMBKKWkoAKKKKBhRRRQAlFLRQAlJS0UwEopcUlABRiiigYlFLSUwCiiigApKWigBKKKKAEopaKYxKMUUUAJRS0YoASiiigBKKWjFAxKQ0tFMBKMUuKSgBKKWjFAxKKKKACkpaKYCYpKWigBKKMUUDEopcUlMBKKWkoA2aKWkrjMAooopgJRS0lABXzp4m+NHifXvETaT4LjMUJkMUDRQiWa4x/FhgQBxngZA6n0+imUOpVuhGDXyPc2PiL4O+O0vBbB1hdxbzSoTFcxkEdexweQOQf1ljRu3Pj74reCbqCXXvOM

... (truncated for length)

---

<Callout icon="pen-to-square" iconType="regular">
  [Edit the source of this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/ollama.mdx)
</Callout>
